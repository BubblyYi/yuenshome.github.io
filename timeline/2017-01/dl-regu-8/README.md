[![header](../../../assets/header21.jpg)](https://yuenshome.github.io)

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>


# 深度学习正则化系列8：对抗训练、切线距离与切线传播和流形切线分类器

《Deep Learning》Chapter 7
Regularization for Deep Learning
翻译水平有限，如有错误请留言，不胜感激！

[toc]
<h1>7.13 对抗训练</h1>
在许多情况下，当在独立同分布的测试集上评估时，神经网络已经开始达到人类的表现性能。因此会自然想知道这些模型对这些任务是否达到同人类一样的理解能力。为了探索网络对底层任务的理解水平，我们可以研究模型分类错误的样本。Szegedy等人（2014b）发现，性能接近人类的神经网络在某些样本点上具有接近 $100％$ 的误差率，这些样本点是使用优化过程搜索到的数据点，例如搜索到与输入点 $x^{'}$ 接近的点是 $x$，使得模型输出与在 $x^{'}$ 样本点处非常不同。在许多情况下， $x^{'}$ 可以非常近似于 $x$ ，人类观察者不能区分原始样本和对抗样本之间的区别，但是网络可以做出高度不同的预测，参见图7.8的例子。

对抗性样本有许多含义，例如超出本章知识范围的计算机安全方面。然而，它在正则化中很有趣，通过对抗训练训练来自训练集的对抗扰动样本，可以减少原始独立同分步的测试集上的错误率（Szegedy等人，2014b；Goodfellow等人，2014b ）。

<!--more-->

<img class="aligncenter" src="http://yuenshome-wordpress.stor.sinaapp.com/uploads/2017/01/deeplearning_chapter7_figure7.8.png" alt="" width="735" height="320" />

<strong>图7.8：在ImageNet上应用于GoogLeNet（Szegedy等人，2014a）生成对抗样本的演示。通过添加一个不可察觉的小向量，其元素等于损失函数相对于输入的梯度的元素的符号（the sign of the elements），我们可以改变GoogLeNet对图像的分类。经Goodfellow等人许可转载（2014b）。</strong>

Goodfellow等人（2014b）表明，这些对抗性样本的主要原因之一是过度的线性。神经网络主要由线性构建块构成。在一些实验中，它们实现的总体功能证明是高度线性的。这些线性函数易于优化。不幸的是，如果线性函数具有许多输入，它的值可以非常快速地改变。如果我们通过 $\epsilon $ 改变每个输入，则具有权重 $w$ 的线性函数值可以被越来越大的 $\epsilon ||w||_1$ 改变，如果 $w$ 是高维的，则计算出的值是非常大的。对抗训练通过鼓励网络局部恒定训练数据的邻域（locally constantin the neighborhood of the training data）来阻止这种高度敏感的局部线性行为。这可以被看作是在监督神经网络中显式地引入局部恒定性（local constancy prior）的方式。

对抗训练有助于说明结合使用大型函数家族和积极正则化力量。纯线性模型，如逻辑回归，不能抵抗对抗样本，因为它们是被强制线性的。神经网络能够表示从接近线性到几乎局部常量值的函数，并因此具有捕获训练数据中线性趋势，并同时学习抵抗局部扰动的灵活性。

对抗样本还提供了完成半监督学习的手段。在与数据集中的标签不相关联的点 $x$ 处，模型本身分配一些标签 $y$ 。模型的标签 $y$ 可能不是真实的标签，但是如果模型是高质量的，则 $y$ 具有提供真实标签的高概率。我们可以选找一个对抗样本，使得分类器输出一个 $y^{'} \neq y$ 的标签 $y$ 。使用不是真实标签而是由训练模型提供的标签产生的对抗性样本被称为<strong>虚拟对抗性样本</strong>（virtual adversarial examples，Miyato等人，2015）。分类器也许会把相同的标签分配给 $x$ 和 $x^{'}$ 。这鼓励分类器学习一个函数，该函数对于未标记数据所在的流形中的任何地方的小变化是鲁棒的。驱动这种方法的假设是不同的类通常位于断开的流形空间上，并且小的扰动不应该能够从一个类的流形空间跳到另一个类的流形空间上。
<h1>7.14 切线距离与切线传播和流形切线分类器</h1>
许多机器学习算法旨在通过假设数据位于低维流行空间附近来克服维数诅咒，如第5.11.3节所述。

早期利用流形假设的尝试之一是切线距离算法（Simard等人，1993，1998）。它是一种非参数最近邻算法，其中使用的度量不是通用欧几里得距离，而是一种从流形空间衍生出的度量方法，接近概率集中附近的流形空间。假设我们试图对实例进行分类，并且处在同一流形空间的样本具有相同的类别。由于分类器应该对流形空间局部的变化因子保持不变，因此使用点 $x_1$ 和 $x_2$ 之间的最近相邻距离作为它们分别属于的流形 $M_1$ 和 $M_2$ 之间的距离是有意义的。虽然在计算上有困难（需要解决优化问题，找到 $M_1$ 和 $M_2$ 上最接近的点对），但是在局部有意义的一种简便的替代方案是通过其在 $x_i$ 处的切平面近似 $M_i$ ，并且测量两个切线，或者在切平面和点之间策略。这可以通过解决低维线性系统（在流形空间的维度中）来实现。当然，该算法需要指定切向量。

在相关方法中，切线传播算法（Simard等人，1992）（图7.9）训练一个具有额外惩罚的神经网络分类器，使得神经网络的每个输出 $f(x)$ 值局部地对变化因子保持不变性。这些变化因子对应于沿着流形的移动，相同类别的样本在该流形空间的附近集中。局部不变性通过要求 $\triangledown_x f(x)$ 与 $x$ 处的已知流形空间的切向量 $v^{(i)}$ 正交来实现，或者等效地，在方向 $v^{(i)}$ 位于样本 $x$ 处的 $f$ 的方向导数，通过增加正则化惩罚 $\Omega$ ：

$$
\begin{align} \label{eq:767}
\Omega(f) = \sum_i \Big((\nabla_{x} f(x)^\top v^{(i)}) \Big)^2 .
\end{align}
$$

这个正则化器当然可以通过适当的超参数缩放，并且对于大多数神经网络，我们将需要对许多输出求和，而不是这里为了简单起见描述的单个输出 $f(x)$ 。与切线距离算法一样，切向量通常是从图像中的变换效果（例如平移，旋转和缩放）的形式知识这样的先验推导出来的。切线传播不仅用于监督学习（Simard等人，1992），而且也适用在强化学习的背景下（Thrun，1995）。

切线传播与数据集扩增（data augmentation）密切相关。在这两种情况下，在不改变网络原本样本的输出情况下，算法使用者将指定一系列数据集图像扩增来获取更多图像数据，并在此过程中考虑该任务的先验知识。不同的是，在数据集增加的情况下，网络被显式地训练以正确地分类通过对原始图像变换从而创建的极少的不同输入。切线传播不需要显式访问新的输入点。相反，它分析地正则化模型以抵抗在与变换相对应方向上的扰动。虽然这种分析方法在理智上是优雅的，但它有两个主要的缺点。首先，它只是将模型正则化以抵抗极少的扰动。显式数据集赋予模型对较大扰动的抵抗力。第二，极少数方法对基于校正线性单元（ReLU）的模型造成了不利影响。这些模型只能通过关闭单元或缩小其权重来收缩其导数。它们不能通过以大的权重以高的值饱和而收缩它们的导数值，如 $S$ 型或 $tanh$ 激活函数。数据集增加可与线性校正单元（ReLU）很好地兼容，因为线性校正单元（ReLU）的不同子集可以激活每个原始输入的不同变换版本。

<img class="aligncenter" src="http://yuenshome-wordpress.stor.sinaapp.com/uploads/2017/01/deeplearning_chapter7_figure7.9.png" alt="" width="384" height="381" />

<strong>图7.9：该图描述了正切传播算法（Simard等人，1992）和流形正切分类器（Rifai等人，2011c）的主要思想，它们都对分类器的输出函数 $f(x)$ 正则化。每条曲线表示不同类别的流形空间，这里显示的是嵌入在二维空间中的一维流形空间。在一个曲线中，我们选择单个点并绘制与该类相切的向量（平行于并且接触流形空间）和垂直于类流形空间（与流形空间正交）的向量。在多个维度中，可以存在许多切线方向和许多法线方向。我们期望分类函数随着其在垂直于流形的方向上移动而快速改变，并且不随着其沿类流形空间移动而改变。切向传播和流形切线分类器正则化 $f(x)$ 不随着 $x$ 沿流形移动而改变很多。流形切线分类器通过训练自动编码器来拟合训练数据来估计流形切线方向，切向传播需要用户手动指定计算切线方向的函数（例如在相同类的流形中指定图像的小平移保持）。使用自动编码器估计流形将在第14章中描述。</strong>

切线传播也与<strong>双重反向传播</strong>（Drucker和LeCun，1992）、对抗训练（Szegedy等人，2014b；Goodfellow等人，2014b）有关。双重反向传播的正则会使Jacobian矩阵变小，而对抗训练在原始输入附近找与之接近的输，并训练模型产生与原始输入一样的输出。切线传播和指定变化方法的数据集扩增这两个策略都要求模型对于输入中的某些指定的变化方向具有一定的不变性。只要变化很小，双重反向传播和对抗训练在模型对输入样本在所有变化方向都有不变性。正如数据集扩增是切线传播的非极少数版本，对抗训练是双重反向传播的非极少数版本（Just as dataset augmentation is the non-inﬁnitesimal version of tangent propagation, adversarial training is the non-inﬁnitesimal version of double backprop.）。

流形切线分类器（Rifai等人，2011c），消除了需要知道切向量的这一先验。如我们将在第14章中看到的，自动编码器可以估计流形切线向量。流形切线分类器使用这种技术，以避免需要用户指定切向量。如图14.10所示，这些估计的切线向量超出了由图像的几何形状（例如平移，旋转和缩放）产生的经典不变量，并且包括因为是特定对象的（例如移动身体部分），也是必须被学习的因素。因此，用流形切线分类提出的算法很简单：（1）使用自动编码器通过无监督学习来学习流形结构，（2）使用这些切线在切线传播中正则化神经网络分类器（方程7.67）。

本章描述了用于正则化神经网络的大多数策略。正则化是机器学习的中心主题，因此其余正则化策略将定期重新讨论。机器学习的另一个中心主题是优化，见下一章节。
