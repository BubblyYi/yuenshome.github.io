[![header](../../../assets/header31.jpg)](https://yuenshome.github.io)

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 深度学习正则化系列4：正则化和受约束问题、数据集扩充、噪声鲁棒性

《Deep Learning》Chapter 7
Regularization for Deep Learning
翻译水平有限，如有错误请留言，不胜感激！

[toc]
<h1>7.3 正则化和受约束问题</h1>
在某些情况下，正则化对机器学习问题的正确定义是必要的。机器学习中的许多线性模型，包括线性回归和主成分分析（PCA）的计算都依赖计算翻转矩阵，即 $X^T X$ 。$X^T X$ 是奇异的是不可能的（This is not possible whenever $X^T X$ is singular）。每当数据生成分布在一些方向上确实没有方差时，或者当在一些方向上没有<strong>观察</strong>到方差时，该矩阵可以是奇异矩阵，因为存在比输入的特征数目（ $X$ 的列）更少的样本数（ $X$ 的行）。在这种情况下，许多形式的正则化对应于反转 $X^T X + \alpha I$ ，这个正则化矩阵保证是可逆的。

<!--more-->

当相关矩阵可逆时，这些线性问题具有闭合形式解。有的问题也可能没有闭合解。一个例子是应用了逻辑回归的问题，问题中数据的类别是线性可分的。如果一个权重向量可以实现完美的分类，那么 $2w$ 会有更大可能性实现完美分类。如随机梯度下降的迭代优化过程将不断更新 $w$ ，并且在理论上将永远不会停止。在实践中，梯度下降的数值实现将最终使权重达到足够大引起数值溢出，此时要做的操作将取决于程序员如何处理不是实数的值。

大多数形式的正则化能够保证应用于不确定问题的迭代方法的收敛。例如，当似然性的斜率等于权重衰减系数时，权重衰减将导致梯度下降使权重不再增大。

使用正则化来解决不确定问题的想法超出了机器学习。相同的想法对于一些基本的线性代数问题也是有用的。

正如我们在第 2.9 节中所看到的，我们可以使用 Moore-Penrose 伪逆来解决不确定的线性方程。回想矩阵 $X$ 的伪逆 $X^+$ 的定义是：

$$
X^+ = \lim_{\alpha \searrow 0} (X^T X + \alpha I)^{-1} X^T.
$$

我们现在可以将该上述公式 7.29 作为具有权重衰减的线性回归。特别地，上述公式 7.29 是方程 7.17 （$w = (X^TX + \alpha I)^{-1}X^Ty$）在正则化系数收缩到零时候的极限情况。因此，我们可以将伪逆解释为使用正则化来稳定未确定的问题。
<h1>7.4 数据集扩增</h1>
使机器学习模型更好地泛化的最好方法基于更多的数据训练 。 当然，在实践中，我们所拥有的数据量有限。 解决这个问题的一种方法是创建假数据并将其添加到训练集，对于一些机器学习任务，创建新数据相当简单。

对于分类问题的数据扩增来说，这种方法是最容易的。 分类器需要采用复杂的高维输入 $x$ ，并用单个类别标识 $y$ 来标记。这意味着分类器的主要任务是对于各种各样的数据变换其输出的结果是不变的。 我们可以通过转换改变训练集中的 $x$ 输入来容易地生成新的 $(x, y)$ 样本对。

这种方法不是很容易适用于许多其他任务。 例如，除非我们已经解决了密度估计问题，否则不可能为密度估计任务生成新的假数据。

对于特定分类问题，如物体识别，数据集扩增方法是一种特别有效的技术。 图像是高维且包括各种变化因素的数据，其中许多因素可以容易地被模拟。即使模型采用第 9 章描述的卷积和池化技术具有局部平移不变性，诸如将训练图像在每个方向上平移几个像素的操作可以很大程度提升泛化性，许多其它操作，如旋转图像或缩放图像在数据扩增中也已被证明是非常有效的。

一个务必注意的是在平移的方法应用中，这种操作会对改变正确的类别。例如，光学字符识别任务需要识别 'b' 和 'd' 之间的差异以及 '6' 和 '9' 之间的差异，因此水平翻转和 180° 旋转不是扩增该任务数据集的适当方式。

还有一些平移方法是我们希望分类器能再见到平移后的数据预测其类别能不变的，但这些方法不容易实现。例如，平面外的旋转操作不能作为对输入像素的简单几何操作来实现。

数据集扩增对语音识别任务也有效（ Jaitlyand Hinton ，2013）。

在神经网络的输入中注入噪声（ Sietsma 和 Dow ，1991）也可以被看作是数据扩增的一种形式。即使小的随机噪声添加到输入，对于许多分类甚至一些回归任务仍然可以被解决。然而，神经网络证明对噪声的鲁棒不是非常好（ Tang 和 Eliasmith ，2010）。提高神经网络的鲁棒性的一种方法是简单地训练时将随机噪声加入到输入中。在输入处将噪声注入是一些无监督学习算法的一部分，如降噪自动编码器（ Vincent et al ，2008）。当噪声到隐藏单元时，噪声注入也起作用，这可以被看作在多个抽象级别处进行数据集增加。 Poole 等人（2014）最近表明，只要仔细调整噪声的幅度，这种方法就可以非常有效。 Dropout 是一个强大的正则化策略，将在 7.12 节中描述，可以看作是通过乘以噪声构建新输入的过程。

当比较机器学习基准测试结果时，考虑数据集增强的效果是很重要的。通常，手动设计的数据集扩充方案可以显着减少机器学习技术的泛化误差。为了比较一种机器学习算法与另一种机器学习算法的性能，有必要进行受控实验。当比较机器学习算法A和机器学习算法B时，有必要确保使用相同的手动设计的数据集增加方案来评估两种算法。假设与输入的许多合成变换组合时，没有数据集扩增的算法A执行得很差，而算法B执行良好。

在这种情况下，将不同的扩增变换方法进行组合可能提升模型分类性能，而不是使用机器学习算法B。有时决定一个实验是否被适当控制需要主观判断，例如将噪声引入输入的机器学习算法的数据集增加的形式。通常可应用的操作（诸如向输入添加高斯噪声）被认为是机器学习算法的一部分，而另一种是特定用在某应用领域的操作（诸如随机地剪切图像）被认为是单独的预处理步骤。
<h1>7.5 噪声鲁棒性</h1>
第 7.4 节提出使用对输入数据加入噪声的方法作为数据集增强策略。 对于一些模型，在模型的输入处添加具有（译注：inﬁnitesimal variance）方差的噪声等效于对权重的范数施加惩罚（Bishop，1995a，b）。在一般情况下，重要的是要记住噪声注入可以比简单地收缩参数更强大，特别是当噪声被添加到隐藏单元时。在隐藏单元处加入噪声是一个重要的话题，值得单独讨论；第 7.12 节中描述的 dropout 算法是该方法的延伸。

在正则化模型中添加噪声的另一种方式是将其添加到权重。这种技术主要用于循环神经网络（Jim et al，1996；Graves，2011）。这可以解释为对权重的贝叶斯推理的随机实现。 贝叶斯学习的处理认为模型权重是不确定的，但可以通过反映这种不确定性的概率分布来表示。将噪声添加到权重是一种实用的且随机的方式来反映不确定性。

将噪声加入到权重上等同于（在一些假设下）更传统的正则化形式，从而增加要学习的函数的稳定性。考虑回归设置，其中我们希望训练使用在模型预测 $\hat{y}(x)$ 和真实值 $\hat{y}(x)$ 之间的最小二乘法成本函数将特征 $x$ 的集合映射到标量的函数 $y$ ：

$$
\begin{align}
J = E_{p(x,y)}[(\hat y(x) - y)^2].
\end{align}
$$

训练集包含 <span class="pl-s"><span class="pl-pds">$</span>m<span class="pl-pds">$ </span></span>对标注样例 <span class="pl-s"><span class="pl-pds">$\{(x^{(1)}, y^{(1)}),\dots,(x^{(m)}, y^{(m)})\}$</span><span class="pl-pds"> 。</span></span>

我们现在假设对于每个输入也包括网络去权重的随机扰动 $\epsilon _W \sim N(\epsilon;0, \eta I )$ 。让我们想象一下有一个标准的 $l$ 层多层感知机（MLP）。 我们将扰动模型表示为 $\hat{y}_{\epsilon_W} (x)$  。除了注入噪声，我们关注的仍是最小化网络输出的平方误差。因此，目标函数变为

$$
\begin{align}
\tilde J_{W}
&amp;= E_{p(x,y,\epsilon_{W})}[(\hat y_{\epsilon_{W}}(x) - y)^2] \\
&amp;= E_{p(x,y,\epsilon_{W})}[\hat y_{\epsilon_{W}}^2(x) - 2y\hat y_{\epsilon_{W}} (x)+ y^2] .
\end{align}
$$

对于较小的 $\eta$ ，对加入了权重噪声（具有协方差 $\eta I$）的损失函数 $J$ 的最小化等价于加入正则化项的 $J$ 的最小化：$ \eta E_{p(x,y)}[||\nabla_{W}~\hat y(x)||^2]$ 。这种形式的正则化会促使参数进入参数空间的区域，在该参数空间中，小的权重扰动对输出具有相对小的影响。换句话说，该形式会使模型处于对小权重的变化也相对敏感的区域，找到的不仅是最小值点，而且是周围被平坦区域包围的最小值点（ Hochreiter 和 Schmidhuber ，1995）。在简化的线性回归模型中（如 $\hat{y}(x) = w^\top x + b$ ），这个正则化项为 $\eta E_{p(x)}[||x||^2]$ ，因为不是参数的函数，因此相对于模型参数不会为损失函数 $\widetilde{J}_W$ 贡献的梯度。
<h2>7.5.1 在输出目标处注入噪声</h2>
大多数数据集在 $y$ 标签中有一些错误，当 $y$ 错误时，对最大化 $log p（y | x）$ 是有害的。防止这种情况的一种方法是对标签上的噪声建模。例如，假设训练集合标签 $y$ 是以概率 $1-\epsilon $ 正确的，其中 $\epsilon $ 是一个小的常数，同时也意味着有 $\epsilon$ 的概率标签 $y$ 是错误的，当前样本标签 $y$ 可能是其他的类型。这个假设很容易被解析地结合到成本函数中，而不是显式地采集噪声样本。

例如，标签平滑基于一个具有 $k$ 个输出值的软性最大分类器（softmax）通过使用 $\frac{\epsilon}{k-1}$ 和 $1-\epsilon$ 分别替代 $0$ 和 $1$ 分类目标来对模型正则化。标准的交叉熵损失可以与软目标结合使用。 使用软性最大（softmax）分类器和硬目标的最大似然学习实际上可能不会收敛——软性最大（softmax）永远不能完全确认这个样本是 $0$ 或 $1$ ，因此它将继续学习到越来越大的权重，从而永远做出更极端的预测。可以使用其它正则化策略（如权重衰减）来防止此情况。标签平滑具有防止在不阻碍正确分类的情况下追求硬概率的优点。这个策略自从1980年代以来一直被使用，并继续在现代神经网络中突出特色（Szegedyet等人，2015）。
