[![header](../../../assets/header38.jpg)](https://yuenshome.github.io)

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 深度学习正则化系列2：参数范数惩罚

《Deep Learning》Chapter 7
Regularization for Deep Learning
翻译水平有限，如有错误请留言，不胜感激！
<h1 class="entry-title">7.1 参数范数惩罚</h1>
在深度学习出现之前，正则化已被使用了十几年。线性模型中的线性回归模型、逻辑斯特回归模型都可以使用简单、直接且有效的正则化方法。

许多正则化方法都是基于有限的模型复杂度，如神经网络模型、线性回归模型、逻辑斯特模型，它们都是通过在目标函数 $J$ 中增加一个参数范数惩罚项 $\Omega (\theta)$ ，我们使用 $\widetilde{J}$ 来表示加入正则化的目标函数：
$$
\widetilde{J} (\theta; X, y) = J(\theta; X, y) + \alpha \Omega (\theta)
$$
其中， $\alpha \in [0, \infty)$ 是一个超参数，用来平衡范数惩罚项 $\Omega$ 的贡献度，也与标准的目标函数 $J$ 有关。若将 $\alpha$ 设置为 $0$ ，那么不存在正则化项。更大的 $\alpha$ 值对应更大的正则力度。[toc]<!--more-->

当训练算法正在对带正则化的目标函数 $\widetilde{J}$ 求最小化时，基于训练数据集的原始目标函数 $J$ 和参数 $\theta$ （或参数的子集）的大小的测量都将会减小。对于参数范数 $\Omega$ 的不同选择将会导致不同的结果作为优选。在本节，我们将会讨论在模型参数上不同范数作为惩罚项的效果。

在深入研究不同范数作为正则的效果前，注意到对于神经网络通常会选择使用一个参数范数惩罚 $\Omega$ ，它只会惩罚每层仿射变换的权重，偏置单元是不会被正则化的。偏置单元在拟合时所需要的数据量要小于权重。

每个权重明确表明两个变量之间是如何相互作用的。要将权重拟合地很好，需要在各种不同的条件下观察这些变量。每一个偏置只会控制一个单独的变量，这也就意味着在保留不被正则化的偏置时，不需要引入过多的方差。 同样，对偏置参数进行正则化会引入相当程度的欠拟合可能。因此我们使用向量 $w$ 来表明所有的权重，这些权重都会被范数惩罚所影响。 其中，向量 $\theta$ 表示所有的参数，既包括了所有的 $w$ 以及没有被正则化的参数。

在神经网络的背景下，对于神经网络的每一层来说。因为在很多超参数的情况下搜索到正确的超参数值，代价很高。所以，在所有层中使用相同的权重衰减策略，用以减少搜索空间的规模，是情有可原的。
<h1>1. $L^2$ 参数正则化</h1>
在 5.5.5 小节中，我们已经看到了一种简单且最常见的参数范数惩罚法： $L^2$ 参数范数惩罚法，也是众所周知的权重衰减（weight decay）。该正则化策略通过在目标函数中加入一个正则化项 $\Omega(\theta) = \frac{1}{2} ||w||_2^2$ 可以使权重越来越接近原本的值（一般来说，我们可以将参数正则化到空间中任何特定值的附近，令人惊讶的是，在收到正则化的作用后，可能会得到一个比之前更好的更接近真实值的结果。零是一个默认值是有意义的，当不知道正确值是正还是负时，零都会作为一个有意义的默认值。因为将模型参数正则化趋向于零是很常见的，我们也将重点关注这种特殊情况）。

在其它一些学术界， $L^2$ 被称为岭回归（ridge regression）或吉洪诺夫正则化（Tikhonov regularization）。

我们深入了解权重衰减的正则化方法，它的实现是通过学习正则化过的目标函数的梯度。为了简化说明，假设不存在偏置参数，所以 $\theta$ 就相当于 $w$ ，该模型有如下的目标函数：

$$
\widetilde{J} (w; X, y) = \frac{\alpha}{2} w^T w + J(w; X, y),
$$

其对应的参数梯度为：

$$
\triangledown_w \widetilde{J}(w;X,y) = \alpha w + \triangledown_w J(w;X,y).
$$

下面是单步梯度的权重更新：

$$
w \leftarrow w - \epsilon (\alpha w + \triangledown_w J(w; X, y)).
$$

也可以换个写法：

$$
w \leftarrow (1 - \epsilon \alpha)w - \epsilon \triangledown_w J(w; X, y).
$$

我们可以看到权重衰减相对学习做了一些修改，在执行这个不同寻常的梯度更新之前，在每一步上借助一个常数因子，对权重向量成倍的进行压缩。这个过程描述了每一步发生了什么，但对于训练的整个过程来说，这又会产生什么影响呢？

我们将通过使用一个二次函数对目标函数进一步对这个分析简化，权重值是未加入正则化项的（基于训练集）损失函数最小值时的权重值，即 $\bf{w}^* = \arg \min_w J(w)$ 。如果目标函数的计算确实是二次形式的，好比用来拟合线性回归模型的基于最小均方误差的二次损失函数，那么这个近似就完美了。其近似 $\widetilde{J}$ 如下：

$$
\widetilde{J}(\theta) = J(w^*) + \frac{1}{2} (w - w^*)^T H (w - w^*),
$$

其中 $H$ 是损失函数 $J$ 关于 $w$ 值为 $w^*$ 时的海森矩阵。在该损失函数的二次近似的式子中并没有一次项，因为 $w^*$ 被定义为损失函数值最小时的取值，即梯度此位置处是不存在的。同样，由于 $w^*$ 所处的位置是 $J$ 的一个极小值点，所以我们也可以说 $H$ 矩阵是半正定矩阵。

$\widetilde{J}$ 的最小值时梯度为：$\triangledown_w \widetilde{J}(w) = H (w - w^*) $ 等于 $0$ 。

为了能研究权重衰减的效果，我们引入一个权重衰减梯度项来修改上面的公式。现在我们就可以求解加入正则项的损失函数 $\widetilde{J}$ 的最小值了，使用变量 $\widetilde{w}$ 来表示损失函数最小值时的权重。

$$
\alpha \widetilde{w} + H (\widetilde{w} - w^*) = 0 \\
(H + \alpha I) \widetilde{w} = H w^* \\
\widetilde{w} = (H + \alpha I)^{-1} H w^*.
$$

随着超参数 $\alpha$ 与 $0$ 越接近，我们索要求解的 $\widetilde{w}$ 也越接近 $w^*$ 。但当超参数 $\alpha$ 增大会发生什么？考虑到矩阵 $H$ 是全对称实矩阵，我们可以使用公式 $H = Q \Lambda Q^T$ 将其分解为对角矩阵 $\Lambda$ 和特征向量的正交基 $Q$ 。将此公式应用到上述最后一个等式 7.10 上，可得：

$$
\begin{align}
\widetilde{w} &amp; = (Q \Lambda Q^T + \alpha I)^{-1} Q \Lambda Q^T w^* \\
&amp; = \left [ Q (\Lambda + \alpha I ) Q^T \right ]^{-1} Q \Lambda Q^T w^* \\
&amp; = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^*.
\end{align}
$$

可以看到权重衰减的作用是对 $w^*$ 沿着 $H$ 的特征向量的轴线方向进行缩放。 $w^*$ 的成分会被一个因子 $\frac{\lambda _i}{\lambda_i + \alpha}$ 与 $H$ 中的第 $i$ 个特征向量进行对准，从而达到缩放的目的。（您也许会好奇这种缩放是如何实现的，可参考我们第一次讲到的图 2.3）。

沿着 $H$ 特征向量轴线的方向，其中 $H$ 的特征值的相当大，有 $\lambda_i &gt;&gt; \alpha$ ，但正则化的效果确实很小的。然而当第 $i$ 个分量有 $\lambda_i &lt;&lt; \alpha$ 时，将会导致缩放到到零这个量级，产生的作用与图 7.1 描述的一样。

<img class="aligncenter" src="./assets/deeplearning_chapter7_figure7.1.png" alt="" width="337" height="343" />
<p style="text-align: center;">图 7.1</p>
图 7.1：该图描述了 $L^2$ （或权重衰减）正则化在最优值 $w$ 时的影响。其中实心椭圆线条表示未加入正则化项的目标函数的等值线，虚线圈表示有着 $L^2$ 正则化的目标函数的等值i线。在点 $\widetilde{w}$ 处，实线条与虚线条相交达到等值。在 $w_1$ 维度的轴线上，损失函数海森矩阵的特征值很小，当水平移动 $w$ 使其距离 $w^*$ 越来越远时，目标函数值并没有明显的增大。因为沿着 $w_1$ 这个水平方向，目标函数没有表现出强烈的偏好，正则化项在这个轴线上有强作用。正则化项拉着 $w_1$ 让它与零越来越近。在 $w_2$ 维度上，目标函数对于 $w$ 远离 $w^*$ 的运动是非常敏感的，这个方向上对应的特征值很大，有着高曲率。这就导致权重衰减在 $w_2$ 这个方向上的影响相当小。

只有能明显地减小目标函数值的参数的方向才能被相对完整保留，在不有助于减小目标函数的方向上，海森矩阵的小特征值告诉我们，在这个方向上的运动不会显着增加梯度。这种不重要方向的权向量分量通过在整个训练中使用正则化来衰减掉。

到目前为止，我们已经讨论了权重衰减在一个抽象的、一般形式以及二次的损失函数形式的优化上产生的作用。那这些作用是如何关系到机器学习中某些方面的呢？我们会发现研究线性回归（其损失函数形式是二次的）的过程也适用于我们目前使用的分析。就训练数据和当前得到的结果再次分析，我们将能够获得相同结果的特殊情况。 对于线性回归，损失函数是误差平方和的形式：

$$
(Xw - y)^T (Xw - y).
$$

当加入 $L^2$ 正则化项，目标函数变为：

$$
(Xw - y)^T (Xw - y) + \frac{1}{2} \alpha w^T w.
$$

这将会改变正规方程（normal equations）的解从

$$
w = (X^TX)^{-1}X^T y
$$

变为

$$
w =  (X^TX + \alpha I)^{-1}X^T y.
$$

在变化前的方程 7.16 中的矩阵 $X^TX$ 是协方差矩阵 $\frac{1}{m} X^T X$ 按照比例计算得到的，使用带 $L^2$ 的正则项 $(X^TX +\alpha I)^{-1}$ 替代原本的 $X^TX$ ，最终得到上述最终变化后的方程 7.17 。新的到的矩阵除了在对角矩阵前加上了一个用于调节大小的超参数 $\alpha$ ，与最初的矩阵是一样的，对角矩阵中每个元素与每个输入特征的方差一一对应。我们可以看到当输入数据的方差比较大时， $L^2$ 正则化可以使学习算法很好地适应输入数据 $X$ ，它可以使得某些对应输出值较小的特征对应的权重得到一定程度的缩放（which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance）。
<h1>2. $L^1$ 正则化</h1>
虽然 $L^2$ 权重衰减是一种常见的权重衰减手段，但还有其它惩罚模型参数规模的方法。另一个选择就是 $L^1$ 正则化方法。

严格地说，模型参数 $w$ 的 $L^1$ 正则化方法定义为：
$$
\Omega(\theta) = ||w||_1 = \sum_i |w_i|
$$

也就是每个参数的绝对值之和。我们首先会研究 $L^1$ 正则化方法在没有偏执参数的线性回归模型上的作用，以及 $L^1$ 与 $L^2$ 两种正则化的不同。与 $L^2$ 权重衰减类似， $L^1$ 权重衰减前面也有一个正的超参数 $\alpha$ 用来控制惩罚权重衰减 $\Omega$ 的力度。加入 $L^1$ 正则后的目标函数 $\widetilde{J} (w; X, y) $ 为：

$$
\widetilde{J} (w; X, y) = \alpha ||w||_1 + J(w;X,y),
$$

对应的梯度计算公式为（实际上是一个子梯度，不是完整梯度的计算形式）：

$$
\triangledown_w \widetilde{J} (w; X, y) = \alpha \text{sign} (w) + \triangledown_w J(X,y;w)
$$

其中 $\text{sign}(w)$ 是对权重向量 $w$ 逐个元素应用符号函数。

通过观察上面得到的最后一个方程 7.20 ，我们可以很快发现 $L^1$ 与 $L^2$ 正则化有很大差别。具体而言，正则对梯度的贡献对于每个 $w_i$ 不再是线性的；而是由一个常数因子 $\alpha$ 和符号函数 $\text{sign}(w_i)$ 组成。这样我们就不必要像研究 $L^2$ 正则化时候那样仔细地研究其代数解做出一个损失函数 $J(X, y; w)$ 的二次形式的近似。

线性模型的损失函数是二次形式的，也可以用泰勒级数表示。此外，可以想象用截断的泰勒级数这样的复杂模型来对这个损失函数进行近似，这样得到的梯度为：

$$
\triangledown_w \hat{J} (w) = H(w - w^*),
$$

其中， $H$ 是损失函数 $J$ 关于 $w$ 取值为 $w^*$ 时的海森矩阵。

因为一般情况下，带上 $L^1$ 惩罚的表达式里面还含有其它项，所以我们进一步假设得到的海森矩阵 $H$ 是对角矩阵，即 $H = \text{diag}([H_{1,1}, ..., H_{n,n}])$ ，其中 $H$ 矩阵中的对角线元素  $H_{i,i} &gt; 0$ 。这条假设保证了线性回归问题的数据已做了去除输入特征之间的所有相关性的预处理，这个预处理过程可能是通过主成分分析完成的。

对于 $L^1$ 正则化的目标函数的二次近似分解成一个基于参数加和的形式：

$$
\hat{J}(w;X,y)=J(w^*;X,y)+\sum_i\left [ \frac{1}{2} H_{i,i} (w_i - w_i^*)^2 + \alpha |w_i| \right ].
$$

最小化近似损失函数的问题有一个分析解（对每一个维度 $i$ ），即如下形式：

$$
w_i = \text{sign}(w_i^*) \max\left \{ |w_i^*| - \frac{\alpha}{H_{i,i}}, 0 \right \}.
$$

考虑到所有的 $i$ 有 $w_i^* &gt; 0$ 的情况，可能有两种结果：
<ol>
	<li>当 $w_i^* \leq \frac{\alpha}{H_{i,i}} $ 时，加入正则化的目标函数最优值时有 $w_i = 0$  。会导致这个的原因是因为 $J(w; X, y)$ 给加入正则化后的目标函数 $\widetilde{J}(w; X, y)$ 的贡献太大了，也就是在方向 $i$ 上 $L^1$ 正则化会将 $w_i$ 的值推向零。</li>
	<li>当 $w_i^* &gt; \frac{\alpha}{H_{i,i}}$ 时，正则化不会将最优值时的 $w_i$ 移动到零，但扔会朝着相同的方向推进 $\frac{\alpha}{H_{i,i}}$ 的距离。</li>
</ol>
当 $w_i^* &lt; 0$ 时，也会发生相似的过程，但 $L^1$ 惩罚会使得 $w_i$ 朝着正数方向移动 $\frac{\alpha}{H_{i,i}}$ 的距离或者移动到 $0$ 。

与 $L^2$ 正则化方法相比， $L^1$ 正则化会产生一个更稀疏的解。在这种背景下产生稀疏解意味着某些参数计算出的解值为零。 $L^1$ 的稀疏性与 $L^2$ 正则化有本质上的不同，公式 7.13 即 $\widetilde{w} = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^*$ 给出了 $L^2$ 正则化时$\widetilde{w}$ 的解，回顾该公式，有一个假设那就是海森矩阵 $H$ 是对角矩阵且是正定矩阵，引入海森矩阵的目的是方便对 $L^1$ 正则化的分析，可以发现 $\widetilde{w_i} = \frac{H_{i,i}}{H_{i,i} + \alpha} w_i^*$ 。如果 $w_i^*$ 是非零的，那么 $\widetilde{w_i}$ 也是非零的。这就论证了即使 $L^1$ 在 $\alpha$ 比较大的情况下也会变为零，但 $L^2$ 正则化不会导致参数变得稀疏。

$L^1$ 正则化的稀疏性已被广泛用于特征选择。特征选择通过从原始的特征中选择出可用的特征子集来简化机器学习问题。特别是众所周知的 LASSO （Tibshirani, 1995）（全称为：least absolute shrinkage and selection operator，即”最小绝对收缩与选择算子“）特征选择模型，它整合了 $L^1$ 的线性模型惩罚和最小平方损失函数。 $L^1$ 惩罚会使得权重向量中的一部分元素值变为零，这也表明这些权重所对应的特征可以被安全地舍弃掉。

在第 5.6.1 小节中，我们看到了很多可以理解为最大后验贝叶斯推断（MAP Bayesian inference）的正则化策略，特别是 $L^2$ 正则化等价于有着高斯分布的先验权重的最大后验（MAP）贝叶斯推断。对于 $L^1$ 正则化来说，当先验是各向同性拉普拉斯分布时（参见第 3.26 小节的方程）即 $w \in \Re^n$ ，用于正则损失函数的惩罚项 $\alpha \Omega (w) = \alpha \sum_i |w_i|$ 等价于由最大后验（MAP）贝叶斯推理最大化的对数先验项。

$$
\log p(w) = \sum_i \log \text{Laplace}(w_i;0,\frac{1}{\alpha}) = -\alpha ||w||_1 + n \log \alpha - n \log2.
$$

从学习的角度来看相对 $w$ 的最大化，我们可以忽略 $\log \alpha - \log 2$ 这两项，因为它们的值不取决于权重 $w$ 。
