[![header](../../../assets/header22.jpg)](https://yuenshome.github.io)

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 深度学习正则化系列1：概述

<p style="text-align: justify;">《Deep Learning》Chapter 7
Regularization for Deep Learning
翻译水平有限，如有错误请留言，不胜感激！</p>
<p style="text-align: justify;">机器学习的一个核心问题是如何使算法在除训练集以外的新输入数据上的性能表现更好。机器学习的不少学习策略都被用来去减少测试误差，有的策略会以牺牲训练误差为代价减少测试误差。这些学习策略都统称为正则化方法。对于深度学习从业者来说，有很多正则化的方法可以使用。实际上在深度学习领域，设计并开发更有效的正则化方法已成为一个主要的研究方向。<!--more--></p>
<p style="text-align: justify;">本书第五章介绍了泛化，欠拟合，过拟合，偏差，方差以及正则化的基本概念。在继续本章的内容之前，如果您还对这些概念不了解，可以参考第五章的具体内容。</p>
<p style="text-align: justify;">本章节将会详细地介绍正则化方法，关注正则化方法在深度模型以及为深度模型搭建提供子模块模型上的应用。</p>
<p style="text-align: justify;">本章的部分小节将会提到机器学习中的标准概念。如果您熟悉这些概念，可以跳过与之相关的小节。但是本章的大部分内容都是这些基本概念的延伸或扩展，尤其是有关神经网络的相关案例。</p>
<p style="text-align: justify;">在 5.2.2 小节给出正则化的定义：“用于修正学习算法减少泛化误差而不是训练误差的策略”。</p>
<p style="text-align: justify;">其中讲到一些正则化方法，某些正则化是基于机器学习模型加入额外限制实现，例如给参数加上限制。某些方法则是在目标函数中加入多余的项，多余的项可以视为对应参数的软性限制。如果正则化方法选择合理，额外的限制和惩罚可以提升模型在测试集上的性能表现。一方面，这些额外的限制和惩罚是作为编码先验知识的一种形式；另一方面来说，这些额外的限制和惩罚被设计出来用于提升模型的泛化能力。有时因为惩罚和限制的存在，可以使原本不确定的问题变得确定。此外，还有其它形式的正则化方法，例如众所周知的集成方法，基于训练数据将多个假设合成一个模型。</p>
<p style="text-align: justify;">在深度学习的背景下，大多数正则化策略都是基于正则化估计。正则化估计通过平衡偏差和方差来实现其作用，如减少方差增大偏差。一个有效的正则可以在显著减少方差的同时不会过分增大偏差。在第五章中讨论泛化和过拟合时，我们主要关注模型族训练过程中的三个情形：（1）排除真实数据产生过程中对应的欠拟合以及偏差增大，（2）真实数据的产生过程，（3）数据生成过程中也可能伴随其它数据的生成过程——导致模型步入过拟合阶段，其中相比偏差，方差是造成了估计误差的主要原因。正则化的目标好比是让模型从第三阶段到第二阶段。</p>
<p style="text-align: justify;">在实际过程中，一个过度复杂的模型族并不需要包括目标函数或者真实数据的产生过程，或与之近似的过程。我们基本上从来都不知道真正数据的产生过程是怎么的，所以我们也就无法确定模型族里是否包含数据的真实生成过程。但是深度学习算法特别地被应用在复杂的领域，如图像、语音序列、文本，因为这些数据的真实产生过程好比模拟整个宇宙。从某种程度上来说，我们总在试图用一些有棱角的零件（即真实数据的产生过程）塞到圆孔（即模型族）中。</p>
控制模型的复杂度不仅意味着发现模型真正的规模大小，还包括参数的规模大小。此外，我们有时也会发现在实际深度学习的应用场景中，（在最小化泛化误差的意义上）拟合最好的模型的特点，不仅是一个大模型，而且也使用了适当的正则化方法。

现在让我们回顾一下如何创建大规模、深层且使用了正则化的模型。
