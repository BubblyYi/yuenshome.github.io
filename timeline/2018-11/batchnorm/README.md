[![header](../../../assets/header29.jpg)](https://yuenshome.github.io)

# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

早先如LeNet、AlexNet、VGG等经典模型都没有BatchNorm这一层，但AlexNet有LRN（一种对于同一个二维像素点跨通道的规范化方法，后来实验发现没有性能提升逐渐舍弃，BatchNorm文中的实验部分也有提到LRN没有精度提升），如果说这些经典神经网络模型中有哪些起到Norm作用的操作，那就应该是数据初始化时减去均值的操作了。

要说数据规范化的定义，来自百度百科的解释是：
- 数据规范化是将原来的度量值转换为无量纲的值。
- 通过将属性数据按比例缩放，通过一个函数将给定属性的整个值域映射到一个新的值域中，即每个旧的值都被一个新的值替代。

常见的数据规范化策略通常有3种：

1. 最小-最大(min-max)规范化：最小-最大规范化保持原有数据之间的联系，即`x_new = (x-x_min)/(x_max-x_min)`。也叫离差标准化，是对原始数据的线性变换，使结果落到`[0,1]`区间，其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义；
2. z-score规范化：当属性`x`的实际最大和最小值未知，或异常点左右了最小－最大规范化时，该方法是有用的，该方法也称为标准差标准化，**经过处理的数据符合标准正态分布，即均值为0，标准差为1，因标准差为1，此时新特征的值就介于[-1,1]之间**，其转化函数为`x_new = (x - E(x)) / std_var(x)`。这里联想到PCA白化，其白化过程的计算就是在PCA的结果上除以标准差。（补：主成分分析，即PCA其作用是考察多个变量间相关性一种多元统计方法，研究如何通过少数几个主成分来揭示多个变量间的内部结构，即从原始变量中导出少数几个主成分，使它们尽可能多地保留原始变量的信息，且彼此间互不相关。通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。其实在PCA操作前也有z-score规范化的过程，计算出标准化的输入再作后续处理）；
3. 小数定标规范化：小数定标规范化通过移动属性A 的小数点位置进行规范化。

## 1. 作者动机

通常来说，对数据做归一化标准化这些操作，都会加快算法的收敛和计算速度。

本文主要介绍的BatchNorm也是类似的操作，下面的内容自某博文对该篇论文的翻译，我加入了一些自己的理解，更详细地说明论文作者想出这个方法的具体动机和原因：

作者认为网络训练过程中参数不断地改变会导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，而一层层累计带来的结果，会让学习到最后一层再反向回传修正参数变得异常困难，因此不得不降低学习率、小心地初始化网络参数。作者将网络每层的feature map分布发生变化称之为 internal covariate shift，即内部协变量漂移，可能没有做规范化的数据经过一层层的线性或非线性变换，导致每层相关性降低，这可能也是造成网络在反向更新参数时，（即使已经跑了一定数量的iteration/epochs）也比较难更新到前面初始的参数（经过大量的迭代次数后，才会对前面几层的网络参数有效果）。

> ### 为什么预处理操作可以加快训练
> 在训练网络的时的一些预处理操作，通常会将输入减去均值（如减去ImageNet的通道均值），还有对输入做白化等操作，目的是为了加快训练。为什么减均值、白化可以加快训练呢，这里做一个简单地说明：
> ![norm-effect](https://user-images.githubusercontent.com/7320657/48112446-707a5200-e291-11e8-9af1-34de6b105b3a.jpg)
> 首先，图像数据是高度相关的，假设其分布如上图a所示（简化为2维）。由于初始化的时候，我们的参数一般都是0均值的，因此开始的拟合`y = k * x + b`，基本过原点附近，如图b红色虚线。因此，网络需要经过多次学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。如果我们对输入数据先作减均值操作，如图c，显然可以加快学习。更进一步的，我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练，**即对原始数据做一个线性核变换，映射到一个更容易区分的空间/维度中去**，如图d。 
参考：解读Batch Normalization - shuzfan的专栏 - CSDN博客
https://blog.csdn.net/shuzfan/article/details/50723877

对于速度的提升，我的另一个理解是：计算机是怎么做乘法的？乘法也是由于二进制的加法来实现的，规范化到`[-1,1]`之间，高位大部分的二进制值为0，自然计算效率提高，有的框架底层的编译器做了优化可能就跳过输入为0的计算，自然变快了。

为了具体了解这个过程，首先我先给出 Caffe 的代码实现，然后再给出其论文中的描述。从中我们也可以看出，学术与工程上的差异。

----

- [规范化_百度百科](https://baike.baidu.com/item/%E8%A7%84%E8%8C%83%E5%8C%96/3193374#2)
- [数据的标准化 | 网站数据分析](http://webdataanalysis.net/data-analysis-method/data-normalization/)
- [【求教】数据规范化中，使用最小-最大规范化和z分数规范化的区别_博问_博客园](https://q.cnblogs.com/q/57003/)
- [二进制十进制间小数怎么转换_百度经验](https://jingyan.baidu.com/article/425e69e6e93ca9be15fc1626.html)
- [计算机是怎么做乘法运算的 - f905699146的博客 - CSDN博客](https://blog.csdn.net/f905699146/article/details/77171372#commentBox)
- [小数怎么以二进制表示？_百度知道](https://zhidao.baidu.com/question/85815874.html)
